{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99883744",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6aa2c42b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.1657e-01, -3.6905e-01, -1.2601e+00,  ...,  2.3323e-01,\n",
      "          -7.1668e-02, -2.0650e-01],\n",
      "         [-4.2603e-01,  5.1927e-01, -1.1892e+00,  ...,  8.8188e-01,\n",
      "           5.2130e-01, -7.2420e-01],\n",
      "         [-1.9452e-01,  1.1631e+00, -3.8681e-01,  ...,  2.9459e-01,\n",
      "           1.3314e+00, -2.3014e-01],\n",
      "         ...,\n",
      "         [-4.6561e-01,  1.4225e-01, -1.3854e+00,  ...,  5.2771e-01,\n",
      "           1.1409e+00, -2.8436e-01],\n",
      "         [-7.0931e-01, -5.6777e-01,  2.4200e+00,  ...,  1.1207e+00,\n",
      "           1.8961e+00, -2.5267e-02],\n",
      "         [ 9.8844e-01,  9.9537e-01,  6.8374e-01,  ...,  2.0445e+00,\n",
      "           2.6545e+00, -3.2404e-01]],\n",
      "\n",
      "        [[ 7.6275e-02, -3.1531e-01, -5.8475e-01,  ..., -4.7538e-01,\n",
      "           3.9984e-01,  4.0923e-01],\n",
      "         [-3.2142e-01,  7.5179e-02,  3.0848e-02,  ...,  4.6124e-01,\n",
      "           5.1557e-01, -9.2494e-01],\n",
      "         [-3.8310e-01,  8.7626e-01, -3.7387e-01,  ..., -8.1742e-02,\n",
      "           9.9139e-01,  2.5840e-01],\n",
      "         ...,\n",
      "         [-8.0533e-01, -2.3614e-01, -1.5247e+00,  ...,  5.6176e-01,\n",
      "           9.1166e-01, -2.8919e-01],\n",
      "         [-9.2721e-01, -8.9247e-01,  1.9308e+00,  ...,  1.0509e+00,\n",
      "           1.5396e+00,  1.2447e-01],\n",
      "         [ 2.0006e+00,  1.4038e+00, -7.9784e-01,  ..., -4.1412e-01,\n",
      "           1.0660e+00, -6.2253e-01]],\n",
      "\n",
      "        [[-3.5950e-01, -2.2462e-01, -1.3804e-01,  ..., -7.3151e-01,\n",
      "           4.3480e-02, -1.3941e-01],\n",
      "         [-4.6045e-01,  1.3437e+00, -6.6123e-01,  ..., -9.4241e-01,\n",
      "           3.6631e-01, -6.4843e-01],\n",
      "         [-9.7633e-02,  6.7209e-01,  7.4608e-02,  ..., -3.2018e-03,\n",
      "           5.7121e-01,  1.0091e-01],\n",
      "         ...,\n",
      "         [-5.9348e-01, -1.0685e-01, -1.1240e+00,  ...,  4.4342e-01,\n",
      "           3.7186e-01, -7.0518e-01],\n",
      "         [-4.3971e-01,  1.9802e-03,  2.3305e+00,  ..., -2.3675e-01,\n",
      "           2.4010e+00,  5.8432e-01],\n",
      "         [ 1.5185e+00,  7.5178e-01,  8.1288e-01,  ...,  1.2724e+00,\n",
      "           2.3294e+00,  1.4909e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.0673e-01, -3.5898e-01, -3.9190e-01,  ..., -7.9007e-02,\n",
      "           1.8930e-01, -1.4846e-02],\n",
      "         [-3.2395e-01,  7.0383e-01, -4.4142e-01,  ...,  9.4667e-01,\n",
      "           4.4885e-01, -8.4763e-01],\n",
      "         [ 1.2323e-01,  4.4440e-01,  3.5603e-02,  ..., -2.5401e-01,\n",
      "           1.4825e+00,  6.5866e-01],\n",
      "         ...,\n",
      "         [-7.6192e-01,  6.5171e-02, -2.2450e+00,  ...,  8.9792e-01,\n",
      "           4.7671e-01,  1.7100e-01],\n",
      "         [-3.6428e-01, -1.1471e+00, -8.5988e-02,  ...,  9.6499e-01,\n",
      "           2.6063e+00,  2.4374e-01],\n",
      "         [ 7.7074e-01,  4.0432e-01, -3.4686e-01,  ...,  2.7532e+00,\n",
      "           2.1128e+00, -8.7471e-02]],\n",
      "\n",
      "        [[ 5.4745e-01,  3.6071e-01, -1.0139e+00,  ...,  2.0992e-01,\n",
      "           4.7042e-01, -2.4398e-01],\n",
      "         [-2.5406e-01,  5.8512e-01,  1.0458e-01,  ..., -8.4756e-01,\n",
      "           2.1768e-01, -8.8362e-01],\n",
      "         [ 1.0978e-03,  1.0600e+00, -2.7759e-01,  ..., -9.0829e-02,\n",
      "           1.3806e+00, -4.1650e-01],\n",
      "         ...,\n",
      "         [-4.0895e-01, -2.6397e-01, -1.2393e+00,  ...,  5.6639e-01,\n",
      "           1.0455e+00,  3.5504e-01],\n",
      "         [-1.0280e+00, -9.7677e-01,  2.0092e+00,  ...,  1.2080e+00,\n",
      "           2.0753e+00,  6.6277e-02],\n",
      "         [ 9.3738e-01,  5.1845e-01,  3.4543e-01,  ..., -6.7876e-01,\n",
      "           2.7202e+00, -4.0498e-01]],\n",
      "\n",
      "        [[-8.7186e-01,  4.2512e-01, -5.4290e-01,  ..., -2.8606e-01,\n",
      "           2.3477e-01, -1.0013e-01],\n",
      "         [-5.0772e-01,  1.3853e+00, -9.9202e-01,  ...,  1.0286e+00,\n",
      "           8.3791e-01, -9.2570e-01],\n",
      "         [ 4.1205e-02,  6.3639e-01,  5.6461e-03,  ...,  6.1254e-01,\n",
      "           9.7606e-01, -5.3357e-01],\n",
      "         ...,\n",
      "         [-3.6662e-01, -2.4966e-01, -1.5365e+00,  ...,  3.1739e-01,\n",
      "           3.5263e-01,  4.3728e-01],\n",
      "         [-6.0793e-01, -1.2332e+00,  1.9760e+00,  ...,  1.0157e+00,\n",
      "           1.8671e+00, -8.1699e-02],\n",
      "         [ 1.1245e+00,  1.3751e+00,  5.8454e-02,  ...,  1.8043e+00,\n",
      "           2.6467e+00, -4.6669e-01]]], grad_fn=<NativeLayerNormBackward0>)\n",
      "Output shape: torch.Size([16, 100, 512])\n"
     ]
    }
   ],
   "source": [
    "#first install torch in independent cell\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = d_model // n_heads\n",
    "\n",
    "        self.query = nn.Linear(d_model, d_model)\n",
    "        self.key = nn.Linear(d_model, d_model)\n",
    "        self.value = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.fc_out = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, query, key, value, mask):\n",
    "        batch_size = query.shape[0]\n",
    "\n",
    "        # Linear transformations\n",
    "        Q = self.query(query)\n",
    "        K = self.key(key)\n",
    "        V = self.value(value)\n",
    "\n",
    "        # Split into heads\n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "\n",
    "        # Scaled Dot-Product Attention\n",
    "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32))\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float('-1e20'))\n",
    "\n",
    "        attention = torch.nn.functional.softmax(energy, dim=-1)\n",
    "        x = torch.matmul(attention, V)\n",
    "\n",
    "        # Reshape and concatenate\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        x = x.view(batch_size, -1, self.d_model)\n",
    "\n",
    "        # Final linear layer\n",
    "        x = self.fc_out(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, ff_dim, dropout=0.1):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.self_attention = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, d_model)\n",
    "        )\n",
    "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
    "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # Self-attention\n",
    "        attention = self.self_attention(x, x, x, mask)\n",
    "        x = x + self.dropout(attention)\n",
    "        x = self.layer_norm1(x)\n",
    "\n",
    "        # Feedforward\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = x + self.dropout(ffn_output)\n",
    "        x = self.layer_norm2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, ff_dim, n_layers, max_seq_length, dropout=0.1):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(d_model, n_heads, ff_dim, dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.position_embedding = nn.Embedding(max_seq_length, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        positions = torch.arange(0, x.size(1)).expand(x.size(0), x.size(1)).to(self.device)\n",
    "        x = x + self.position_embedding(positions)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Example usage:\n",
    "d_model = 512\n",
    "n_heads = 8\n",
    "ff_dim = 2048\n",
    "n_layers = 6\n",
    "max_seq_length = 100\n",
    "dropout = 0.1\n",
    "\n",
    "# Create transformer encoder\n",
    "transformer_encoder = TransformerEncoder(d_model, n_heads, ff_dim, n_layers, max_seq_length, dropout)\n",
    "\n",
    "# Dummy input\n",
    "input_data = torch.rand((16, 100, d_model))\n",
    "\n",
    "# Mask for padding\n",
    "padding_mask = (input_data.sum(dim=-1) != 0).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "# Forward pass\n",
    "output_data = transformer_encoder(input_data, padding_mask)\n",
    "print(output_data)\n",
    "print(\"Output shape:\", output_data.shape)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5efaa845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition: A deep learning model architecture based on self-attention, widely used in NLP tasks.\n",
    "# Key Components:\n",
    "# Self-Attention: Computes attention scores among input tokens.\n",
    "# Positional Encoding: Adds positional information to input tokens.\n",
    "# Multi-Head Attention: Allows the model to focus on different parts of the input simultaneously.\n",
    "# Feed-Forward Networks: Apply additional transformation layers.\n",
    "# Layer Normalization: Normalizes the output of each layer to stabilize training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ef2922",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
